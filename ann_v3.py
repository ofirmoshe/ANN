import math
import numpy as np
from sklearn.model_selection import train_test_split
import pickle
import json
from typing import List
from sklearn.utils import shuffle


class ANN:

    @staticmethod
    def initialize_parameters(layer_dims: List[int]) -> dict:
        """
        :param layer_dims: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input,
        layer L is the output
        softmax)
        :return: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).
        """

        np.random.seed(1)
        parameters = {}
        L = len(layer_dims)

        for l in range(1, L):
            parameters[f"W{l}"] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2 / layer_dims[l - 1])
            parameters[f"b{l}"] = np.zeros((layer_dims[l], 1))

        return parameters

    @staticmethod
    def linear_forward(A, W, b):
        """
        Implement the linear part of a layer's forward propagation.
        :param A: the activations of the previous layer
        :param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
        :param b: the bias vector of the current layer (of shape [size of current layer, 1])
        :return: Z – the linear component of the activation function (i.e., the value before applying the non-linear function)
        linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)
        """
        res = W.dot(A) + b
        linear_cache = A, W, b
        return res, linear_cache

    @staticmethod
    def softmax(Z):
        """
        :param Z: the linear component of the activation function
        :return: – the activations of the layer.
        activation_cache – returns Z, which will be useful for the backpropagation
        """
        new_z = np.array(Z, copy=True)
        new_z -= np.max(Z, axis=0, keepdims=True)
        return np.exp(new_z) / np.sum(np.exp(new_z), axis=0, keepdims=True), Z

    @staticmethod
    def relu(Z):
        """
        :param Z: the linear component of the activation function
        :return: A – the activations of the layer
        activation_cache – returns Z, which will be useful for the backpropagation
        """
        A = np.maximum(Z, 0)
        return A, Z

    def linear_activation_forward(self, A_prev, W, B, activation):
        """
        Implement the forward propagation for the LINEAR->ACTIVATION layer
        :param A_prev: activations of the previous layer
        :param W: the weights matrix of the current layer
        :param B: the bias vector of the current layer
        :param activation: the activation function to be used (a string, either “softmax” or “relu”)
        :return: A – the activations of the current layer
        cache – a joint dictionary containing both linear_cache and activation_cache
        """
        z, linear_cache = self.linear_forward(A=A_prev, W=W, b=B)
        if activation == 'softmax':
            A, activation_cache = self.softmax(Z=z)
        elif activation == 'relu':
            A, activation_cache = self.relu(Z=z)
        else:
            raise KeyError(f"ERROR: wrong activation function - {activation}, please use 'relu' or 'softmax'")

        return A, (linear_cache, activation_cache)

    def l_model_forward(self, X, parameters, use_batchnorm=False):
        """
        Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation
        :param X: the data, numpy array of shape (input size, number of examples)
        :param parameters: the initialized W and b parameters of each layer
        :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation (note that this option needs to be set
        to “false” in Section 3 and “true” in Section 4)
        :return: AL – the last post-activation value
        caches – a list of all the cache objects generated by the linear_forward function
        """
        L = int(len(parameters.keys()) / 2)
        A = X
        cache = []
        for layer_idx in range(1, L):
            A, curr_cache = self.linear_activation_forward(A_prev=A, W=parameters[f"W{layer_idx}"],
                                                           B=parameters[f"b{layer_idx}"], activation="relu")
            cache.append(curr_cache)
            if use_batchnorm:
                A = self.apply_bachnorm(A)

        AL, curr_cache = self.linear_activation_forward(A_prev=A, W=parameters[f"W{L}"],
                                                   B=parameters[f"b{L}"], activation="softmax")
        cache.append(curr_cache)
        return AL, cache

    @staticmethod
    def compute_cost(AL, Y):
        """
        Implement the cost function defined by equation. The requested cost function is categorical cross-entropy loss. The formula is as follows:
        cost=-1/m*∑_1^m▒∑_1^C▒〖y_i  log⁡〖(y ̂)〗 〗, where y_i is one for the true class (“ground-truth”) and y ̂ is the softmax-adjusted prediction
        (this link provides a good overview).
        :param AL: probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
        :param Y: the labels vector (i.e. the ground truth)
        :return: cost – the cross-entropy cost
        """
        if len(AL.shape) == 1:
            m = 1
        else:
            m = AL.shape[-1]

        cost = np.sum(np.log(AL + 1e-12) * Y) / -m
        return cost

    @staticmethod
    def apply_bachnorm(A):
        """
        performs batchnorm on the received activation values of a given layer
        :param A: the activation values of a given layer
        :return: NA - the normalized activation values, based on the formula learned in class
        """
        eps = 1e-8
        mean = np.mean(A, axis=1, keepdims=True)
        var = np.sum((A - mean) ** 2, axis=1, keepdims=True) / A.shape[-1]
        NA = (A - mean) / (np.sqrt(var + eps))
        return NA

    @staticmethod
    def apply_dropout(A, dropout):
        """
        A - the activation values of a given layer
        dropout - probability to keep a neuron
        returns:
            A_d - the activation values of a given layer after zeroing dropout% from the outputs.
        """
        A_d = np.array(A, copy=True)
        A_d[np.random.rand(*A.shape) > dropout] = 0
        A_d /= dropout
        return A_d

    @staticmethod
    def linear_backward(dZ, cache):
        """
        Implements the linear part of the backward propagation process for a single layer
        :param dZ: the gradient of the cost with respect to the linear output of the current layer (layer l)
        :param cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
        :return: dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW -- Gradient of the cost with respect to W (current layer l), same shape as W
        db -- Gradient of the cost with respect to b (current layer l), same shape as b
        """
        A_prev, W, b = cache
        m = A_prev.shape[1]
        dW = (1 / m) * dZ.dot(A_prev.T)
        db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)
        dA_prev = W.T.dot(dZ)
        return dA_prev, dW, db

    def linear_activation_backward(self, dA, cache, activation, Y):
        """
        Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then applies the linear_backward
        function.
        Some comments:
             - The derivative of ReLU is f^' (x)={■(1&if x>0@0&otherwise)┤
             - The derivative of the softmax function is: p_i-y_i, where p_i is the softmax-adjusted probability of the class and y_i is the
             “ground truth” (i.e. 1 for the real class, 0 for all others)
             - You should use the activations cache created earlier for the calculation of the activation derivative and the linear cache should be
             fed to the linear_backward function

        :param dA: post activation gradient of the current layer
        :param cache: contains both the linear cache and the activations cache
        :return: dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
        dW – Gradient of the cost with respect to W (current layer l), same shape as W
        db – Gradient of the cost with respect to b (current layer l), same shape as b
        """
        linear_cache, activation_cache = cache
        if activation == 'softmax':
            dZ = self.softmax_backward(dA=dA, Y=Y)
        elif activation == 'relu':
            dZ = self.relu_backward(dA=dA, activation_cache=activation_cache)
        else:
            raise KeyError(f"ERROR: wrong activation function - {activation}, please use 'relu' or 'softmax'")

        return self.linear_backward(dZ=dZ, cache=linear_cache)

    @staticmethod
    def relu_backward(dA, activation_cache):
        """
        Implements backward propagation for a ReLU unit
        :param dA: the post-activation gradient
        :param activation_cache: contains Z (stored during the forward propagation)
        :return: dZ – gradient of the cost with respect to Z
        """
        z = activation_cache
        dz = pickle.loads(pickle.dumps(dA))
        dz[z <= 0] = 0
        return dz

    @staticmethod
    def softmax_backward(dA, Y):
        """
        Implements backward propagation for a softmax unit
        :param dA: the post-activation gradient
        :return: dZ – gradient of the cost with respect to Z
        """
        dZ = dA - Y
        return dZ

    def l_model_backward(self, AL, Y, caches):
        """
        Implement the backward propagation process for the entire network
        Some comments:
        the backpropagation for the softmax function should be done only once as only the output layers uses it and the RELU should be done
        iteratively over all the remaining layers of the network.
        :param AL: the probabilities vector, the output of the forward propagation (L_model_forward)
        :param Y: the true labels vector (the "ground truth" - true classifications)
        :param caches: list of caches containing for each layer: a) the linear cache; b) the activation cache
        :return:
            Grads - a dictionary with the gradients
                 grads["dA" + str(l)] = ...
                 grads["dW" + str(l)] = ...
                 grads["db" + str(l)] = ...
        """
        grads = {}
        last = True
        for layer_idx, cache in reversed(list(enumerate(caches))):
            if last:
                # use softmax
                activation = 'softmax'
                c_dA = AL
                last = False
            else:
                c_dA = grads[f"dA{layer_idx + 2}"]
                activation = 'relu'
            grads[f"dA{layer_idx + 1}"], grads[f"dW{layer_idx + 1}"], grads[f"db{layer_idx + 1}"] = self.linear_activation_backward(c_dA, cache, activation, Y)
        return grads

    @staticmethod
    def update_parameters(parameters, grads, learning_rate):
        """
        Updates parameters using gradient descent
        :param parameters: a python dictionary containing the DNN architecture’s parameters
        :param grads: a python dictionary containing the gradients (generated by L_model_backward)
        :param learning_rate: the learning rate used to update the parameters (the “alpha”)
        :return: parameters – the updated values of the parameters object provided as input
        """
        L = int(len(parameters.keys()) / 2)

        for l in range(L):
            parameters["W" + str(l + 1)] = parameters["W" + str(l + 1)] - learning_rate * grads["dW" + str(l + 1)]
            parameters["b" + str(l + 1)] = parameters["b" + str(l + 1)] - learning_rate * grads["db" + str(l + 1)]
        return parameters

    def l_layer_model(self, X, Y, layers_dims, learning_rate, batch_size, use_batchnorm=False):
        """
        Implements a L-layer neural network. All layers but the last should have the ReLU activation function, and the final layer will apply the
        softmax activation function. The size of the output layer should be equal to the number of labels in the data. Please select a batch size
        that enables your code to run well (i.e. no memory overflows while still running relatively fast).

        Hint: the function should use the earlier functions in the following order: initialize -> L_model_forward -> compute_cost ->
        L_model_backward -> update parameters

        :param X: the input data, a numpy array of shape (height*width , number_of_examples)
                Comment: since the input is in grayscale we only have height and width, otherwise it would have been height*width*3
        :param Y: the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
        :param layers_dims: a list containing the dimensions of each layer, including the input
        :param learning_rate:
        :param batch_size: the number of examples in a single training batch
        :return:
        parameters – the parameters learnt by the system during the training (the same parameters that were updated in the update_parameters
        function).
        costs – the values of the cost function (calculated by the compute_cost function). One value is to be saved after each 100 training
        iterations (e.g. 3000 iterations -> 30 values)
        """

        X_train, X_val, Y_train, Y_val = train_test_split(X.T, Y.T, test_size=0.20)
        X_train, X_val, Y_train, Y_val = X_train.T, X_val.T, Y_train.T, Y_val.T
        parameters = self.initialize_parameters(layers_dims)
        val_accuracy_scores = []
        train_accuracy_scores = []
        val_loss = []
        train_loss = []
        epochs = []
        val_acc_last_100_all = [math.inf]
        i = 0
        epoch = 0
        val_acc_last_100 = 0
        max_diff = None
        while True:
            X, Y = shuffle(X_train.T, Y_train.T)
            X = X.T
            Y = Y.T
            for j in range(0, X.shape[-1], batch_size):
                X_curr, Y_curr = X[:, j:j + batch_size], Y[:, j:j + batch_size]

                AL, caches = self.l_model_forward(X_curr, parameters, use_batchnorm)
                grads = self.l_model_backward(AL, Y_curr, caches)
                parameters = self.update_parameters(parameters, grads, learning_rate)

                val_accuracy_scores.append(self.predict(X_val, Y_val, parameters, use_batchnorm))
                train_accuracy_scores.append(self.predict(X_train, Y_train, parameters, use_batchnorm))
                # Calculate stopping criteria
                if len(val_accuracy_scores) > 100:
                    val_acc_last_100 = np.mean(val_accuracy_scores[-100:])
                    max_diff = abs(max(val_accuracy_scores[-100:]) - min(val_accuracy_scores[-100:]))

                # Check if validation scores has converged
                if max_diff and max_diff < 0.005 and epoch > 0:
                    print(f"Validation accuracy is converged in iteration {i}, epoch {epoch}.")
                    print("========= Final results: =========")
                    print(f"Epoch: {epoch}, Iteration: {i}\n"
                          f"Last validation score: {val_accuracy_scores[-1]}, Last validation loss: {val_loss[-1]}\n"
                          f"Last train score: {train_accuracy_scores[-1]}, Last train loss: {train_loss[-1]}\n"
                          f"Last 100 validation scores mean: {val_acc_last_100}, Max diff: {max_diff}\n")
                    with open("stats.json", mode='w') as f:
                        json.dump(dict(epochs=epochs, val_acc_last_100_all=val_acc_last_100_all, train_loss=train_loss, val_loss=val_loss), f)

                    return parameters, train_loss

                if i % 100 == 0:
                    cost = self.compute_cost(AL, Y_curr)
                    train_loss.append(cost)

                    AL_val, _ = self.l_model_forward(X_val, parameters, use_batchnorm)

                    cost = self.compute_cost(AL_val, Y_val)
                    val_loss.append(cost)

                    epochs.append(epoch)
                    val_acc_last_100_all.append(val_acc_last_100)
                    print(f"Epoch: {epoch}, Iteration: {i}\n"
                          f"Last validation score: {val_accuracy_scores[-1]}, Last validation loss: {val_loss[-1]}\n"
                          f"Last train score: {train_accuracy_scores[-1]}, Last train loss: {train_loss[-1]}\n"
                          f"Last 100 validation scores mean: {val_acc_last_100}, Max diff: {max_diff}\n")
                i += 1

            print(f"Epoch {epoch} done, Iteration:{i}\n"
                  f"Last validation score: {val_accuracy_scores[-1]}, Last validation loss: {val_loss[-1]}\n"
                  f"Last train score: {train_accuracy_scores[-1]}, Last train loss: {train_loss[-1]}\n"
                  f"Last 100 validation scores mean: {val_acc_last_100}, Max diff: {max_diff}\n")
            epoch += 1

    def predict(self, X, Y, parameters, use_batchnorm=False):
        """
        The function receives an input data and the true labels and calculates the accuracy of the trained neural network on the data
        :param X: the input data, a numpy array of shape (height*width, number_of_examples)
        :param Y: the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
        :param parameters: a python dictionary containing the DNN architecture’s parameters
        :return: accuracy – the accuracy measure of the neural net on the provided data (i.e. the percentage of the samples for which the correct
        label receives the highest confidence score). Use the softmax function to normalize the output values.
        """
        output, _ = self.l_model_forward(X, parameters, use_batchnorm=use_batchnorm)
        y_real = np.argmax(Y, axis=0)
        y_pred = np.argmax(output, axis=0)
        return np.sum(y_pred == y_real) / len(y_pred)